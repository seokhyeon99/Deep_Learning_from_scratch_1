{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4. Neural Network Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning means getting the optimized weight values from train data by minimizing the value of loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 learn from data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of image classification,  \n",
    "Machine learning such as SVM, KNN, etc: train the pattern of the features extracted from the images. However, the features are still selected by human.  \n",
    "Neural Network (Deep Learning): Machine chooses and extracts the important features from the images for itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate universal ability, we normally deal with learning machine by dividing the data into train data and test data. (universal ability stands for the ability that can solve the problmes machine never met before.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nerual network find the optimized weight and bias values by minimizing loss function. i.e. Mean squared error(MSE) and cross entropy error(CEE) are usually used as the loss function.  \n",
    "The reason why we can get the optimized values based on accuracy is accuracy has many points where the differentiated value is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mean squared error(MSE) for one data\n",
    "  \n",
    "    $ E =  \\frac{1}{2} \\Sigma_k (y_k - t_k)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5*np.sum((y-t)**2)\n",
    "\n",
    "# Example\n",
    "y = [0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.6,0.0,0.0]\n",
    "t = [0,0,1,0,0,0,0,0,0,0] #one hot encoing\n",
    "\n",
    "mean_squared_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross entropy error(CEE) for one data\n",
    "  \n",
    "    $ E = - \\Sigma_k t_k log y_k$ ($log$ is natural logarithm: $log_e$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2968435295135659"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-7 #very tiny value\n",
    "    return -np.sum(y*np.log(y + delta)) #To prevent the denominator from being 0 and the result from being -inf\n",
    "\n",
    "# Example\n",
    "y = [0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.6,0.0,0.0]\n",
    "t = [0,0,1,0,0,0,0,0,0,0] #one hot encoing\n",
    "\n",
    "cross_entropy_error(np.array(y),np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Above formulas are only for one data. The below is loss function for the whole data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross entropy error(CEE) for whole data  \n",
    "    $E = -\\frac{1}{N}\\Sigma_n\\Sigma_kt_{nk}logy_{nk}$  \n",
    "    ($log$ is natural logarithm($log_e$) and  $t_nk$ is $k^{th}$value of $n^{th}$data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mini-batch\n",
    "    train some of the datas in neural network learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784')\n",
    "import numpy as np\n",
    "\n",
    "X = mnist['data']\n",
    "T = mnist['target']\n",
    "\n",
    "x_train = X[:60000]\n",
    "t_train = T[:60000]\n",
    "x_test = X[60000:]\n",
    "t_test = T[60000:]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross entropy error(CEE) for mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error_minibatch(y,t): #one hot encoding\n",
    "    if y.ndim == 1: #make the array to 2-d\n",
    "        y = y.reshape(1,y.size)\n",
    "        t = t.reshape(1,t.size)\n",
    "    \n",
    "    delta = 1e-7\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t*np.log(y+delta))/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entrpy_error_minibatch_1(y,t): #when the answer array consists of number label(not one hot enocoding)\n",
    "    if y.ndim == 1: #make the array to 2-d\n",
    "        y = y.reshape(1,y.size)\n",
    "        t = t.reshape(1,t.size)\n",
    "    \n",
    "    delta = 1e-7\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size),t]+delta))/batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is not changed by a tiny correction. Though accuracy respond to the small correction, the value of accuracy is changed discontinuously. Thus, we can not use accuracy as an indicator. It is similar to the reason why we don't use step function as an activation function. On the other hand, slope of Sigmoid function is changed continuously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Numerical Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- differentiation\n",
    "$$ \\frac{df(x)} {dx} = lim_{h->0} \\frac{f(x+h)-f(x)}{h} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f,x):\n",
    "    h = 1e-4 #0.0001\n",
    "    return (f(x+h) - f(x-h))/(2*h) #central difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- partial derivative  \n",
    "we use it when there are more than or equal to 2 variables.  \n",
    "e.g. $ \\frac {\\delta f}{\\delta x_0} $  \n",
    "we set a target variable among several variables and differentiate the formula considering other variables except for the target variable as constants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient is a vector representation of the partial derivatives of all variables. e.g. $(\\frac {\\delta f}{\\delta x_0},\\frac {\\delta f}{\\delta x_1})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def numerical_gradient(f,x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    print('x size', x.size)\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2)/(2*h)\n",
    "        x[idx] = tmp_val\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gradient method(gradient descent)  \n",
    "    Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. The idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent. Conversely, stepping in the direction of the gradient will lead to a local maximum of that function; the procedure is then known as gradient ascent.  [reference: https://en.wikipedia.org/wiki/Gradient_descent ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ x_0 = x_0 - \\eta \\frac{\\delta f}{\\delta x_0} $$  \n",
    "$$ x_1 = x_1 - \\eta \\frac{\\delta f}{\\delta x_1} $$\n",
    "$\\eta$ is the learning rate that stands for the amount that the parameters are updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f,x)\n",
    "        x -= lr*grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e.g. $f(x_0, x_1) = x_0^2+x_1^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function(x): \n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n",
      "x size 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function, init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call parameter such as learning rate hyper parameter. While weight parameter of neural network is the automatically calculated parameter, hyper parameters like learning rate should be set by a human. Thus, we have to find the optimized learning rate by testing various values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ W = \\left[\\begin{array}{rrr} \n",
    "w_{11}&w_{12}&w_{13}\\\\\n",
    "w_{21}&w_{22}&w_{23}\\\\\n",
    "\\end{array}\\right]$ #weights\n",
    "\n",
    "$ \\frac{\\delta L}{\\delta W} = \\left[\\begin{array}{rrr} \n",
    "\\frac {\\delta L}{\\delta w_{11}} & \\frac {\\delta L}{\\delta w_{12}} & \\frac {\\delta L}{\\delta w_{13}}\\\\\n",
    "\\frac {\\delta L}{\\delta w_{21}} & \\frac {\\delta L}{\\delta w_{22}} & \\frac {\\delta L}{\\delta w_{23}}\\\\\n",
    "\\end{array}\\right]$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e. get numerical gradient of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functions import softmax, cross_entropy_error\n",
    "from gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y,t)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.25071009 -0.27777949 -0.08377893]\n",
      " [ 0.02727741 -0.97571362 -0.74860081]]\n",
      "[ 0.17497573 -1.04480995 -0.72400809]\n",
      "max index: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.430949848036708"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "print('max index:', np.argmax(p))\n",
    "t = np.array([0, 0, 1])\n",
    "net.loss(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.35246904  0.1040818  -0.45655084]\n",
      " [ 0.52870356  0.1561227  -0.68482625]]\n"
     ]
    }
   ],
   "source": [
    "def f(w): # f = lambda w: net.loss(x,t)\n",
    "    return net.loss(x,t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Learning Algorithm implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network have adaptible weights and bias. Learning is modulating them according to the train data.  \n",
    "- $1^{st}$stage - minibatch  \n",
    "- $2^{nd}$stage - gradient calculation  \n",
    "- $3^{rd}$stage - parameter updates  \n",
    "- $4^{th}$stage - repetition  \n",
    "\n",
    "We call this process stochastic gradient descent(SGD) because we extract data randomly using minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "from gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        #initiate weights\n",
    "        self.params ={}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        a1 = np.dot(x, W1)+b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2)+b2\n",
    "        y = softmax(a2)\n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t): #x: input data, t:answer data\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        #t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t)/float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x,t) #p138 ?\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784')\n",
    "\n",
    "X = mnist['data']\n",
    "T = mnist['target']\n",
    "\n",
    "x_train = X[:60000]\n",
    "t_train = T[:60000]\n",
    "x_test = X[60000:]\n",
    "t_test = T[60000:]\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "#hyper parameter\n",
    "iters_num = 10\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    print(i)\n",
    "    #mini-batch\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    t_batch = np.array(list(map(int, t_batch))) #change the type of data in target batch from str to int\n",
    "    \n",
    "    #gradient\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    #grad = network.gradient(x_batch, t_batch) #upgrade version\n",
    "    \n",
    "    #update parameters\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    #train-loss\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x1 = np.arange(0,10,1)\n",
    "y1 = train_loss_list\n",
    "plt.scatter(x1,y1)\n",
    "plt.xlabel('number')\n",
    "plt.ylabel('loss')\n",
    "plt.title('visualization of loss change')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows the value of loss function decreases as the number of learning increase. It means weight parameter of neural network is changed adjusting to data more and more. When we make computer train data, we should make sure that the training makes overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "train acc: 0.14123333333333332 / test acc: 0.1403\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seokhyeonlee/Desktop/Source/Deep Learning from scratch 1/functions.py:15: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "train acc: 0.8464833333333334 / test acc: 0.8505\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "train acc: 0.8603 / test acc: 0.8652\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "train acc: 0.8505333333333334 / test acc: 0.856\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n",
      "1862\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784')\n",
    "\n",
    "X = mnist['data']\n",
    "T = mnist['target']\n",
    "\n",
    "x_train = X[:60000]\n",
    "t_train = T[:60000]\n",
    "x_test = X[60000:]\n",
    "t_test = T[60000:]\n",
    "\n",
    "#change the type of target data from str to int\n",
    "t_train = np.array(list(map(int, t_train))) \n",
    "t_test = np.array(list(map(int, t_test))) \n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "#hyper parameter\n",
    "iters_num = 2000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "\n",
    "iter_per_epoch = max(train_size/batch_size, 1)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    print(i)\n",
    "    #mini-batch\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    #t_batch = np.array(list(map(int, t_batch))) #change the type of data in target batch from str to int\n",
    "    \n",
    "    #gradient\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    #grad = network.gradient(x_batch, t_batch) #upgrade version\n",
    "    \n",
    "    #update parameters\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    #train-loss\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    #accuracy per epoch\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print('train acc:', str(train_acc), '/ test acc:', str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 0.8594833333333334 / test acc: 0.8664\n"
     ]
    }
   ],
   "source": [
    "#2000-iter accuracy\n",
    "train_acc = network.accuracy(x_train, t_train)\n",
    "test_acc = network.accuracy(x_test, t_test)\n",
    "train_acc_list.append(train_acc)\n",
    "test_acc_list.append(test_acc)\n",
    "print('train acc:', str(train_acc), '/ test acc:', str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x1 = np.arange(0,5,1)\n",
    "x2 = np.arange(0,5,1)\n",
    "y1 = train_acc_list\n",
    "y2 = test_acc_list\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(x1,y1)\n",
    "plt.xlabel('number')\n",
    "plt.ylabel('train accuracy')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(x2,y2)\n",
    "plt.xlabel('number')\n",
    "plt.ylabel('test accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
