{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4. Neural Network Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning means getting the optimized weight values from train data by minimizing the value of loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 learn from data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of image classification,  \n",
    "Machine learning such as SVM, KNN, etc: train the pattern of the features extracted from the images. However, the features are still selected by human.  \n",
    "Neural Network (Deep Learning): Machine chooses and extracts the important features from the images for itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate universal ability, we normally deal with learning machine by dividing the data into train data and test data. (universal ability stands for the ability that can solve the problmes machine never met before.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nerual network find the optimized weight and bias values by minimizing loss function. i.e. Mean squared error(MSE) and cross entropy error(CEE) are usually used as the loss function.  \n",
    "The reason why we can get the optimized values based on accuracy is accuracy has many points where the differentiated value is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mean squared error(MSE) for one data\n",
    "  \n",
    "    $ E =  \\frac{1}{2} \\Sigma_k (y_k - t_k)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5*np.sum((y-t)**2)\n",
    "\n",
    "# Example\n",
    "y = [0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.6,0.0,0.0]\n",
    "t = [0,0,1,0,0,0,0,0,0,0] #one hot encoing\n",
    "\n",
    "mean_squared_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross entropy error(CEE) for one data\n",
    "  \n",
    "    $ E = - \\Sigma_k t_k log y_k$ ($log$ is natural logarithm: $log_e$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2968435295135659"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-7 #very tiny value\n",
    "    return -np.sum(y*np.log(y + delta)) #To prevent the denominator from being 0 and the result from being -inf\n",
    "\n",
    "# Example\n",
    "y = [0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.6,0.0,0.0]\n",
    "t = [0,0,1,0,0,0,0,0,0,0] #one hot encoing\n",
    "\n",
    "cross_entropy_error(np.array(y),np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Above formulas are only for one data. The below is loss function for the whole data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross entropy error(CEE) for whole data  \n",
    "    $E = -\\frac{1}{N}\\Sigma_n\\Sigma_kt_{nk}logy_{nk}$  \n",
    "    ($log$ is natural logarithm($log_e$) and  $t_nk$ is $k^{th}$value of $n^{th}$data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mini-batch\n",
    "    train some of the datas in neural network learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784')\n",
    "import numpy as np\n",
    "\n",
    "X = mnist['data']\n",
    "T = mnist['target']\n",
    "\n",
    "x_train = X[:60000]\n",
    "t_train = T[:60000]\n",
    "x_test = X[60000:]\n",
    "t_test = T[60000:]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross entropy error(CEE) for mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error_minibatch(y,t): #one hot encoding\n",
    "    if y.ndim == 1: #make the array to 2-d\n",
    "        y = y.reshape(1,y.size)\n",
    "        t = t.reshape(1,t.size)\n",
    "    \n",
    "    delta = 1e-7\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t*np.log(y+delta))/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entrpy_error_minibatch_1(y,t): #when the answer array consists of number label(not one hot enocoding)\n",
    "    if y.ndim == 1: #make the array to 2-d\n",
    "        y = y.reshape(1,y.size)\n",
    "        t = t.reshape(1,t.size)\n",
    "    \n",
    "    delta = 1e-7\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size),t]+delta))/batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is not changed by a tiny correction. Though accuracy respond to the small correction, the value of accuracy is changed discontinuously. Thus, we can not use accuracy as an indicator. It is similar to the reason why we don't use step function as an activation function. On the other hand, slope of Sigmoid function is changed continuously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Numerical Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- differentiation\n",
    "$$ \\frac{df(x)} {dx} = lim_{h->0} \\frac{f(x+h)-f(x)}{h} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f,x):\n",
    "    h = 1e-4 #0.0001\n",
    "    return (f(x+h) - f(x-h))/(2*h) #central difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- partial derivative  \n",
    "we use it when there are more than or equal to 2 variables.  \n",
    "e.g. $ \\frac {\\delta f}{\\delta x_0} $  \n",
    "we set a target variable among several variables and differentiate the formula considering other variables except for the target variable as constants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient is a vector representation of the partial derivatives of all variables. e.g. $(\\frac {\\delta f}{\\delta x_0},\\frac {\\delta f}{\\delta x_1})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def numerical_gradient(f,x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    print('x size', x.size)\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2)/(2*h)\n",
    "        x[idx] = tmp_val\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gradient method(gradient descent)  \n",
    "    Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. The idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent. Conversely, stepping in the direction of the gradient will lead to a local maximum of that function; the procedure is then known as gradient ascent.  [reference: https://en.wikipedia.org/wiki/Gradient_descent ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ x_0 = x_0 - \\eta \\frac{\\delta f}{\\delta x_0} $$  \n",
    "$$ x_1 = x_1 - \\eta \\frac{\\delta f}{\\delta x_1} $$\n",
    "$\\eta$ is the learning rate that stands for the amount that the parameters are updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f,x)\n",
    "        x -= lr*grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e.g. $f(x_0, x_1) = x_0^2+x_1^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function(x): \n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np #p132\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function, init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call parameter such as learning rate hyper parameter. While weight parameter of neural network is the automatically calculated parameter, hyper parameters like learning rate should be set by a human. Thus, we have to find the optimized learning rate by testing various values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ W = \\left[\\begin{array}{rrr} \n",
    "w_{11}&w_{12}&w_{13}\\\\\n",
    "w_{21}&w_{22}&w_{23}\\\\\n",
    "\\end{array}\\right]$ #weights\n",
    "\n",
    "$ \\frac{\\delta L}{\\delta W} = \\left[\\begin{array}{rrr} \n",
    "\\frac {\\delta L}{\\delta w_{11}} & \\frac {\\delta L}{\\delta w_{12}} & \\frac {\\delta L}{\\delta w_{13}}\\\\\n",
    "\\frac {\\delta L}{\\delta w_{21}} & \\frac {\\delta L}{\\delta w_{22}} & \\frac {\\delta L}{\\delta w_{23}}\\\\\n",
    "\\end{array}\\right]$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e. get numerical gradient of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functions import softmax, cross_entropy_error\n",
    "from gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y,t)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.1354359   2.06728318 -1.90040207]\n",
      " [-1.14902074  0.11314514  0.2198908 ]]\n",
      "[-1.11538021  1.34220054 -0.94233953]\n",
      "max index: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.4563578054010984"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "print('max index:', np.argmax(p))\n",
    "t = np.array([0, 0, 1])\n",
    "net.loss(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.043273    0.50527841 -0.54855141]\n",
      " [ 0.0649095   0.75791761 -0.82282711]]\n"
     ]
    }
   ],
   "source": [
    "def f(W): # f = lambda w: net.loss(x,t)\n",
    "    return net.loss(x,t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Learning Algorithm implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network have adaptible weights and bias. Learning is modulating them according to the train data.  \n",
    "- $1^{st}$stage - minibatch  \n",
    "- $2^{nd}$stage - gradient calculation  \n",
    "- $3^{rd}$stage - parameter updates  \n",
    "- $4^{th}$stage - repetition  \n",
    "\n",
    "We call this process stochastic gradient descent(SGD) because we extract data randomly using minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "from gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        #initiate weights\n",
    "        self.params ={}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        a1 = np.dot(x, W1)+b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2)+b2\n",
    "        y = softmax(a2)\n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t): #x: input data, t:answer data\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t)/float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x,t)\n",
    "        #p 138"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
